/// <reference types="vite/client" />

/**
 * VAD ÂΩïÈü≥ Hook - ‰ΩøÁî® FunASR FSMN-VAD ONNX
 * 
 * ÁâπÁÇπÔºö
 * - ‰ΩøÁî®ÈòøÈáåËææÊë©Èô¢ FunASR FSMN-VAD Ê®°Âûã
 * - 80Áª¥ Fbank ÁâπÂæÅ + 5Â∏ß LFR
 * - CMVN ÂΩí‰∏ÄÂåñ
 * - ÂÆûÊó∂ËØ≠Èü≥Ê¥ªÂä®Ê£ÄÊµã
 */
import { useState, useRef, useCallback, useEffect } from 'react';
// import { FunASRVAD } from '../services/funasrVAD'; // Removed VAD
import { float32ToWav, float32ToBase64 } from '../utils/audioHelper';
import { createLogger } from '../utils/loggerRenderer';




// Define VAD Modes
export type VADMode = 'vad' | 'time_limit' | 'unlimited';

interface VADRecordingResult {
    isRecording: boolean;
    isVADReady: boolean;
    isSpeaking: boolean;
    chunkCount: number;
    duration: number;
    error: string | null;
    sessionId: string | null;
    startRecording: () => Promise<boolean>;
    stopRecording: () => void;
    onChunkReady: (callback: (chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) => void;
    stream: MediaStream | null; // Shared stream for Visualizer
}

// VAD ÈÖçÁΩÆ
// VAD Config removed
// const VAD_CONFIG = { ... }

// Èü≥È¢ëÈÖçÁΩÆ
// Èü≥È¢ëÈÖçÁΩÆ
const SAMPLE_RATE = 16000;
// ScriptProcessorNode bufferSize ÂøÖÈ°ªÊòØ 2 ÁöÑÂπÇÊ¨°Êñπ (256, 512, 1024, 2048, 4096, 8192, 16384)
const BUFFER_SIZE = 2048; // ~128ms @ 16kHz
const BUFFER_SIZE_MS = Math.floor(BUFFER_SIZE * 1000 / SAMPLE_RATE); // 256ms

const logger = createLogger('VADRecording');

export function useVADRecording(
    initialMode: VADMode = 'unlimited',
    initialTimeLimitMs: number = 180000
): VADRecordingResult & { setVadMode: (mode: VADMode) => void, setTimeLimit: (ms: number) => void } {
    // Current Mode State
    const [, _setVadMode] = useState<VADMode>(initialMode);
    const vadModeRef = useRef<VADMode>(initialMode);
    const timeLimitRef = useRef<number>(initialTimeLimitMs);

    const setVadMode = useCallback((mode: VADMode) => {
        const oldMode = vadModeRef.current;
        console.log(`üîÑ [VAD Mode Change] ${oldMode} ‚Üí ${mode}`);
        _setVadMode(mode);
        vadModeRef.current = mode;
        console.log(`‚úÖ [VAD Mode Updated] vadModeRef.current is now: ${vadModeRef.current}`);
    }, []);

    const setTimeLimit = useCallback((ms: number) => {
        console.log(`Updated Time Limit to: ${ms}ms`);
        timeLimitRef.current = ms;
    }, []);

    const [isRecording, setIsRecording] = useState(false);
    const [isVADReady, setIsVADReady] = useState(false);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const isSpeakingRef = useRef(false); // Ref to avoid closure trap in ScriptProcessor callback
    const [chunkCount, setChunkCount] = useState(0);
    const [duration, setDuration] = useState(0);
    const [error, setError] = useState<string | null>(null);
    const [sessionId, setSessionId] = useState<string | null>(null);
    const sessionIdRef = useRef<string | null>(null);

    // const vadRef = useRef<FunASRVAD | null>(null); // Removed
    const streamRef = useRef<MediaStream | null>(null);
    const [streamState, setStreamState] = useState<MediaStream | null>(null); // Reactive state for stream sharing
    const audioContextRef = useRef<AudioContext | null>(null);
    const processorRef = useRef<ScriptProcessorNode | null>(null);

    const chunkCallbackRef = useRef<((chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) | null>(null);
    const chunkIndexRef = useRef(0);
    const durationTimerRef = useRef<number | null>(null);

    // VAD Áä∂ÊÄÅÁºìÂÜ≤
    const speechBufferRef = useRef<Float32Array[]>([]);
    const silenceFramesRef = useRef(0);
    const speechFramesRef = useRef(0);

    // Debug Logging State
    const lastLogTimeRef = useRef<number>(0);

    const logDebug = useCallback((message: string) => {
        window.ipcRenderer.invoke('write-debug-log', message);
    }, []);

    // Ê≥®ÂÜåÂõûË∞É
    const onChunkReady = useCallback((callback: (chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) => {
        chunkCallbackRef.current = callback;
    }, []);

    // ÂàùÂßãÂåñ VAD
    // VAD Initialization Removed
    useEffect(() => {
        setIsVADReady(true); // Always ready as we don't need model
        return () => { };
    }, []);

    // Â§ÑÁêÜÈü≥È¢ëÂùó
    const processAudioChunk = useCallback(async (inputBuffer: Float32Array) => {
        // if (!vadRef.current?.ready) return; // Removed check

        try {
            // Calculate Amplitude for frequent logging
            let maxAmp = 0;
            for (let i = 0; i < inputBuffer.length; i++) {
                const abs = Math.abs(inputBuffer[i]);
                if (abs > maxAmp) maxAmp = abs;
            }

            // Ëé∑ÂèñÊ®°ÂºèÈÖçÁΩÆ
            const vadMode = vadModeRef.current;
            const now = Date.now();

            // Log flow every 100ms
            if (now - lastLogTimeRef.current > 100) {
                logDebug(`[AudioFlow] Amp=${maxAmp.toFixed(4)} | Mode=${vadMode} | Buffer=${speechBufferRef.current.length} | isSpeaking=${isSpeakingRef.current}`);
                lastLogTimeRef.current = now;
            }

            // 1. Always append to Temp Recording
            if (sessionIdRef.current) {
                const base64Coords = float32ToBase64(inputBuffer);
                window.ipcRenderer.invoke('append-temp-recording', sessionIdRef.current, base64Coords)
                    .catch(e => console.error('Temp write failed:', e));
            }

            // --- CONTINUOUS MODES (Unlimited & Time Limit) ---
            // --- CONTINUOUS MODES (Unlimited & Time Limit) ---
            // Treated VAD mode as Unlimited for safety if somehow selected
            if (vadMode === 'unlimited' || vadMode === 'time_limit' || vadMode === 'vad') {
                speechBufferRef.current.push(inputBuffer.slice());

                // Always mark as speaking in continuous modes so UI shows activity
                if (!isSpeaking) {
                    setIsSpeaking(true);
                    isSpeakingRef.current = true;
                }

                // Check Time Limit
                if (vadMode === 'time_limit') {
                    const currentBufferDurationMs = speechBufferRef.current.length * BUFFER_SIZE_MS;
                    if (currentBufferDurationMs >= timeLimitRef.current) {
                        logDebug(`[TimeLimit] Limit met (${timeLimitRef.current}ms) -> CUTTING`);

                        // Perform Cut
                        const totalLength = speechBufferRef.current.reduce((sum, buf) => sum + buf.length, 0);
                        const mergedAudio = new Float32Array(totalLength);
                        let offset = 0;
                        for (const buf of speechBufferRef.current) {
                            mergedAudio.set(buf, offset);
                            offset += buf.length;
                        }

                        const wavBuffer = float32ToWav(mergedAudio, SAMPLE_RATE);
                        const currentIndex = chunkIndexRef.current;
                        chunkIndexRef.current++;
                        setChunkCount((prev: number) => prev + 1);

                        logDebug(`[TimeLimit] Flushed Chunk #${currentIndex} | Size=${wavBuffer.byteLength} | Dur=${(mergedAudio.length / SAMPLE_RATE).toFixed(2)}s`);

                        if (chunkCallbackRef.current) {
                            chunkCallbackRef.current(currentIndex, wavBuffer, mergedAudio);
                        }

                        speechBufferRef.current = [];
                    }
                }

                // Log less frequently
                if (now - lastLogTimeRef.current > 1000) {
                    logDebug(`[Continuous] Mode=${vadMode} | Buffering: ${speechBufferRef.current.length} chunks`);
                }
                return;
            }

            // --- VAD MODE REMOVED ---
            // If we reach here, it's an unknown mode, but we handled 'vad' above as fallback.
        } catch (err) {
            console.error('VAD Â§ÑÁêÜÈîôËØØ:', err);
            logDebug(`[Error] VAD Processing: ${err}`);
        }
    }, [isSpeaking, logDebug]);

    // ÂºÄÂßãÂΩïÈü≥
    const startRecording = useCallback(async (): Promise<boolean> => {
        if (!isVADReady) {
            setError('VAD Ê®°ÂûãÂ∞öÊú™Âä†ËΩΩ');
            return false;
        }

        try {
            setError(null);
            chunkIndexRef.current = 0;
            setChunkCount(0);
            setDuration(0);
            speechBufferRef.current = [];
            silenceFramesRef.current = 0;
            speechFramesRef.current = 0;

            // Ëé∑ÂèñÈ∫¶ÂÖãÈ£é
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    sampleRate: SAMPLE_RATE,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                }
            });
            streamRef.current = stream;
            setStreamState(stream); // Trigger re-render to share stream with Waveform

            // Start Temp Recording Session
            try {
                const sessionRes = await window.ipcRenderer.invoke('start-temp-recording');
                if (sessionRes.success && sessionRes.sessionId) {
                    sessionIdRef.current = sessionRes.sessionId;
                    setSessionId(sessionRes.sessionId);
                    logger.info('Temp recording session started', { sessionId: sessionRes.sessionId });
                } else {
                    console.error('Failed to start temp recording:', sessionRes.error);
                }
            } catch (e) {
                console.error('IPC error starting temp recording:', e);
            }

            // ÂàõÂª∫ AudioContext
            const audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            audioContextRef.current = audioContext;

            const source = audioContext.createMediaStreamSource(stream);

            // ‰ΩøÁî® ScriptProcessorNode ËøõË°åÂÆûÊó∂Â§ÑÁêÜ
            const processor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (event) => {
                const inputData = event.inputBuffer.getChannelData(0);
                processAudioChunk(new Float32Array(inputData));
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            setIsRecording(true);

            // ÂêØÂä®Êó∂ÈïøËÆ°Êó∂Âô®
            const startTime = Date.now();
            durationTimerRef.current = window.setInterval(() => {
                setDuration(Math.floor((Date.now() - startTime) / 1000));
            }, 1000);

            console.log('üé§ FunASR VAD ÂΩïÈü≥Â∑≤ÂêØÂä®');
            logger.info('Recording started', { mode: vadModeRef.current, sampleRate: SAMPLE_RATE });
            return true;

        } catch (err) {
            logger.error('Failed to start recording', err as Error);
            setError('È∫¶ÂÖãÈ£éËÆøÈóÆÂ§±Ë¥•: ' + (err as Error).message);
            return false;
        }
    }, [isVADReady, processAudioChunk]);

    // ÂÅúÊ≠¢ÂΩïÈü≥
    const stopRecording = useCallback(() => {
        if (durationTimerRef.current) {
            clearInterval(durationTimerRef.current);
            durationTimerRef.current = null;
        }

        if (processorRef.current) {
            processorRef.current.disconnect();
            processorRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
            setStreamState(null); // Clear shared stream state
        }

        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        // Â§ÑÁêÜÂâ©‰ΩôÁöÑËØ≠Èü≥ÁºìÂÜ≤
        if (speechBufferRef.current.length > 0) {
            const totalLength = speechBufferRef.current.reduce((sum, buf) => sum + buf.length, 0);
            const mergedAudio = new Float32Array(totalLength);
            let offset = 0;
            for (const buf of speechBufferRef.current) {
                mergedAudio.set(buf, offset);
                offset += buf.length;
            }

            // CHECK MODE: If unlimited or time_limit, do not slice. Send ONE BIG CHUNK.
            const currentMode = vadModeRef.current;

            if (currentMode === 'unlimited' || currentMode === 'time_limit') {
                // --- CONTINUOUS MODES: SINGLE CHUNK ---
                const wavBuffer = float32ToWav(mergedAudio, SAMPLE_RATE);
                const currentIndex = chunkIndexRef.current;
                chunkIndexRef.current++;
                setChunkCount((prev: number) => prev + 1);

                console.log(`üéµ [${currentMode}] ÂèëÈÄÅÂâ©‰ΩôÂΩïÈü≥, Â§ßÂ∞è: ${wavBuffer.byteLength} bytes, Êó∂Èïø: ${(mergedAudio.length / SAMPLE_RATE).toFixed(2)}s`);
                logger.info('Sending remaining recording', {
                    mode: currentMode,
                    size: wavBuffer.byteLength,
                    duration: (mergedAudio.length / SAMPLE_RATE).toFixed(2)
                });

                if (chunkCallbackRef.current) {
                    chunkCallbackRef.current(currentIndex, wavBuffer, mergedAudio);
                }

            } else {
                // --- VAD MODE: Keep safety slicing if very long (fallback) ---
                // VAD should have cut already. This is just cleanup for leftovers.

                // Slice into smaller chunks for transmission (e.g. 10s = 16000 * 10 = 160000 samples)
                const MAX_CHUNK_SAMPLES = 16000 * 10; // 10 seconds per chunk

                for (let i = 0; i < mergedAudio.length; i += MAX_CHUNK_SAMPLES) {
                    const end = Math.min(i + MAX_CHUNK_SAMPLES, mergedAudio.length);
                    const chunkPCM = mergedAudio.slice(i, end);
                    const wavBuffer = float32ToWav(chunkPCM, SAMPLE_RATE);

                    const currentIndex = chunkIndexRef.current;
                    chunkIndexRef.current++;
                    setChunkCount((prev: number) => prev + 1);

                    console.log(`üéµ [Cleanup] ÂèëÈÄÅÂâ©‰ΩôÈü≥È¢ëÁâáÊÆµ #${currentIndex}, Â§ßÂ∞è: ${wavBuffer.byteLength} bytes`);

                    if (chunkCallbackRef.current) {
                        chunkCallbackRef.current(currentIndex, wavBuffer, chunkPCM);
                    }
                }
            }

            speechBufferRef.current = [];
        } else {
            console.warn('‚ö†Ô∏è ÂÅúÊ≠¢ÂΩïÈü≥Êó∂ÁºìÂÜ≤Âå∫‰∏∫Á©∫ (VAD ÂèØËÉΩÊú™Ê£ÄÊµãÂà∞ËØ≠Èü≥)');
        }
        // vadRef.current?.reset(); // Removed
        setIsRecording(false);
        setIsSpeaking(false);
        isSpeakingRef.current = false;

        logger.info('Recording stopped', { chunkCount: chunkIndexRef.current });
        console.log('üõë ÂΩïÈü≥Â∑≤ÂÅúÊ≠¢');

        sessionIdRef.current = null;
    }, []);

    // Ê∏ÖÁêÜ
    useEffect(() => {
        return () => {
            if (durationTimerRef.current) {
                clearInterval(durationTimerRef.current);
            }
            streamRef.current?.getTracks().forEach(track => track.stop());
            audioContextRef.current?.close();
        };
    }, []);

    return {
        isRecording,
        isVADReady,
        isSpeaking,
        chunkCount,
        duration,
        error,
        startRecording,
        stopRecording,
        onChunkReady,
        setVadMode,
        setTimeLimit,
        sessionId,
        stream: streamState // Reactive state for Waveform visualization
    };
}
