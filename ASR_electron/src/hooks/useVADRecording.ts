/// <reference types="vite/client" />

/**
 * VAD å½•éŸ³ Hook - ä½¿ç”¨ FunASR FSMN-VAD ONNX
 * 
 * ç‰¹ç‚¹ï¼š
 * - ä½¿ç”¨é˜¿é‡Œè¾¾æ‘©é™¢ FunASR FSMN-VAD æ¨¡å‹
 * - 80ç»´ Fbank ç‰¹å¾ + 5å¸§ LFR
 * - CMVN å½’ä¸€åŒ–
 * - å®æ—¶è¯­éŸ³æ´»åŠ¨æ£€æµ‹
 */
import { useState, useRef, useCallback, useEffect } from 'react';
import { FunASRVAD } from '../services/funasrVAD';
import { float32ToWav, float32ToBase64 } from '../utils/audioHelper';

// DEBUG SESSION ID
const currentDebugSessionId = 'debug_session_' + Date.now();


// Define VAD Modes
export type VADMode = 'vad' | 'time_limit' | 'unlimited';

interface VADRecordingResult {
    isRecording: boolean;
    isVADReady: boolean;
    isSpeaking: boolean;
    chunkCount: number;
    duration: number;
    error: string | null;
    sessionId: string | null;
    startRecording: () => Promise<boolean>;
    stopRecording: () => void;
    onChunkReady: (callback: (chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) => void;
    stream: MediaStream | null; // Shared stream for Visualizer
}

// VAD é…ç½®
const VAD_CONFIG = {
    speechThreshold: 0.1,       // è¯­éŸ³æ£€æµ‹é˜ˆå€¼ (é™ä½ä»¥æé«˜çµæ•åº¦)
    silenceThreshold: 0.35,     // é™éŸ³æ£€æµ‹é˜ˆå€¼
    minSpeechDurationMs: 200,   // æœ€å°è¯­éŸ³æ®µ
    minSilenceDurationMs: 500,  // é™éŸ³è§¦å‘åˆ‡åˆ†
};

// éŸ³é¢‘é…ç½®
// éŸ³é¢‘é…ç½®
const SAMPLE_RATE = 16000;
// ScriptProcessorNode bufferSize å¿…é¡»æ˜¯ 2 çš„å¹‚æ¬¡æ–¹ (256, 512, 1024, 2048, 4096, 8192, 16384)
const BUFFER_SIZE = 2048; // ~128ms @ 16kHz
const BUFFER_SIZE_MS = Math.floor(BUFFER_SIZE * 1000 / SAMPLE_RATE); // 256ms

export function useVADRecording(
    initialMode: VADMode = 'unlimited',
    initialTimeLimitMs: number = 180000
): VADRecordingResult & { setVadMode: (mode: VADMode) => void, setTimeLimit: (ms: number) => void } {
    // Current Mode State
    const [, _setVadMode] = useState<VADMode>(initialMode);
    const vadModeRef = useRef<VADMode>(initialMode);
    const timeLimitRef = useRef<number>(initialTimeLimitMs);

    const setVadMode = useCallback((mode: VADMode) => {
        const oldMode = vadModeRef.current;
        console.log(`ğŸ”„ [VAD Mode Change] ${oldMode} â†’ ${mode}`);
        _setVadMode(mode);
        vadModeRef.current = mode;
        console.log(`âœ… [VAD Mode Updated] vadModeRef.current is now: ${vadModeRef.current}`);
    }, []);

    const setTimeLimit = useCallback((ms: number) => {
        console.log(`Updated Time Limit to: ${ms}ms`);
        timeLimitRef.current = ms;
    }, []);

    const [isRecording, setIsRecording] = useState(false);
    const [isVADReady, setIsVADReady] = useState(false);
    const [isSpeaking, setIsSpeaking] = useState(false);
    const isSpeakingRef = useRef(false); // Ref to avoid closure trap in ScriptProcessor callback
    const [chunkCount, setChunkCount] = useState(0);
    const [duration, setDuration] = useState(0);
    const [error, setError] = useState<string | null>(null);
    const [sessionId, setSessionId] = useState<string | null>(null);
    const sessionIdRef = useRef<string | null>(null);

    const vadRef = useRef<FunASRVAD | null>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const [streamState, setStreamState] = useState<MediaStream | null>(null); // Reactive state for stream sharing
    const audioContextRef = useRef<AudioContext | null>(null);
    const processorRef = useRef<ScriptProcessorNode | null>(null);

    const chunkCallbackRef = useRef<((chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) | null>(null);
    const chunkIndexRef = useRef(0);
    const durationTimerRef = useRef<number | null>(null);

    // VAD çŠ¶æ€ç¼“å†²
    const speechBufferRef = useRef<Float32Array[]>([]);
    const silenceFramesRef = useRef(0);
    const speechFramesRef = useRef(0);

    // Debug Logging State
    const lastLogTimeRef = useRef<number>(0);

    const logDebug = useCallback((message: string) => {
        window.ipcRenderer.invoke('write-debug-log', message);
    }, []);

    // æ³¨å†Œå›è°ƒ
    const onChunkReady = useCallback((callback: (chunkIndex: number, audioData: ArrayBuffer, rawPCM: Float32Array) => void) => {
        chunkCallbackRef.current = callback;
    }, []);

    // åˆå§‹åŒ– VAD
    useEffect(() => {
        let isMounted = true;
        const initVAD = async () => {
            // Prevent double init if already ready or processing
            if (vadRef.current) return;

            try {
                console.log('ğŸ”„ åˆå§‹åŒ– FunASR VAD...');
                const vad = new FunASRVAD(VAD_CONFIG);
                await vad.init('/models/model.onnx', '/models/vad.mvn');

                if (isMounted) {
                    vadRef.current = vad;
                    setIsVADReady(true);
                    console.log('âœ… FunASR VAD åˆå§‹åŒ–å®Œæˆ');
                } else {
                    vad.reset(); // Cleanup if unmounted during init
                }
            } catch (err) {
                if (isMounted) {
                    console.error('FunASR VAD åˆå§‹åŒ–å¤±è´¥:', err);
                    setError('VAD æ¨¡å‹åŠ è½½å¤±è´¥: ' + (err as Error).message);
                }
            }
        };

        initVAD();

        return () => {
            isMounted = false;
            // Only reset if we own the instance
            if (vadRef.current) {
                vadRef.current.reset();
                vadRef.current = null;
            }
        };
    }, []);

    // å¤„ç†éŸ³é¢‘å—
    const processAudioChunk = useCallback(async (inputBuffer: Float32Array) => {
        // --- DEBUG CAPTURE VAD INPUT ---
        try {
            const base64 = float32ToBase64(inputBuffer);
            window.ipcRenderer.invoke('save-debug-audio-file', currentDebugSessionId, 'vad', base64);
        } catch (e) {
            console.error('Debug save failed', e);
        }
        // -------------------------------

        if (!vadRef.current?.ready) return;

        try {
            // Calculate Amplitude for frequent logging
            let maxAmp = 0;
            for (let i = 0; i < inputBuffer.length; i++) {
                const abs = Math.abs(inputBuffer[i]);
                if (abs > maxAmp) maxAmp = abs;
            }

            // è·å–æ¨¡å¼é…ç½®
            const vadMode = vadModeRef.current;
            const now = Date.now();

            // Log flow every 100ms
            if (now - lastLogTimeRef.current > 100) {
                logDebug(`[AudioFlow] Amp=${maxAmp.toFixed(4)} | Mode=${vadMode} | Buffer=${speechBufferRef.current.length} | isSpeaking=${isSpeakingRef.current}`);
                lastLogTimeRef.current = now;
            }

            // 1. Always append to Temp Recording
            if (sessionIdRef.current) {
                const base64Coords = float32ToBase64(inputBuffer);
                window.ipcRenderer.invoke('append-temp-recording', sessionIdRef.current, base64Coords)
                    .catch(e => console.error('Temp write failed:', e));
            }

            // --- UNLIMITED MODE ---
            if (vadMode === 'unlimited') {
                speechBufferRef.current.push(inputBuffer.slice());
                if (!isSpeaking) {
                    setIsSpeaking(true);
                    isSpeakingRef.current = true;
                }
                // Log that we are just buffering
                if (now - lastLogTimeRef.current > 1000) { // Log less frequently in unlimited
                    logDebug(`[Unlimited] Buffering... Total chunks: ${speechBufferRef.current.length}`);
                }
                return;
            }

            // --- VAD MODE ---
            const speechProb = await vadRef.current.detect(inputBuffer);

            // Log VAD result
            logDebug(`[VAD-Detect] Prob=${speechProb.toFixed(4)} | Amp=${maxAmp.toFixed(4)}`);

            if (speechProb < 0) {
                logDebug(`[VAD-Skip] Insufficient data`);
                if (vadMode === 'vad' || vadMode === 'time_limit') {
                    speechBufferRef.current.push(inputBuffer.slice());
                }
                return;
            }

            if (speechProb >= VAD_CONFIG.speechThreshold) {
                speechFramesRef.current++;
                silenceFramesRef.current = 0;

                if (!isSpeakingRef.current && speechFramesRef.current >= 2) {
                    logDebug(`[VAD-Event] START SPEAKING (Prob=${speechProb.toFixed(3)})`);
                    setIsSpeaking(true);
                    isSpeakingRef.current = true;
                }
                speechBufferRef.current.push(inputBuffer.slice());
            } else if (speechProb < VAD_CONFIG.silenceThreshold) {
                silenceFramesRef.current++;
                speechFramesRef.current = 0;

                if (vadMode === 'time_limit' || vadMode === 'vad') {
                    speechBufferRef.current.push(inputBuffer.slice());
                }

                const silenceDurationMs = silenceFramesRef.current * BUFFER_SIZE_MS;
                let shouldCut = false;

                if (vadMode === 'vad') {
                    if (silenceDurationMs >= VAD_CONFIG.minSilenceDurationMs && isSpeakingRef.current) {
                        logDebug(`[VAD-Event] SILENCE DETECTED (${silenceDurationMs}ms) -> CUTTING`);
                        shouldCut = true;
                        setIsSpeaking(false);
                        isSpeakingRef.current = false;
                    } else if (silenceDurationMs >= VAD_CONFIG.minSilenceDurationMs) {
                        logDebug(`[VAD-Event] Silence threshold met (${silenceDurationMs}ms) but not speaking, no cut.`);
                    }
                } else if (vadMode === 'time_limit') {
                    const currentBufferDurationMs = speechBufferRef.current.length * BUFFER_SIZE_MS;
                    if (currentBufferDurationMs >= timeLimitRef.current) {
                        logDebug(`[VAD-Event] TIME LIMIT REACHED (${timeLimitRef.current}ms) -> CUTTING`);
                        shouldCut = true;
                    }
                }

                if (shouldCut) {
                    if (speechBufferRef.current.length > 0) {
                        // åˆå¹¶è¯­éŸ³ç¼“å†²å¹¶å‘é€
                        const totalLength = speechBufferRef.current.reduce((sum, buf) => sum + buf.length, 0);
                        const mergedAudio = new Float32Array(totalLength);
                        let offset = 0;
                        for (const buf of speechBufferRef.current) {
                            mergedAudio.set(buf, offset);
                            offset += buf.length;
                        }

                        const wavBuffer = float32ToWav(mergedAudio, SAMPLE_RATE);
                        const currentIndex = chunkIndexRef.current;
                        chunkIndexRef.current++;
                        setChunkCount((prev: number) => prev + 1);

                        logDebug(`[Chunk-Cut] Sending Chunk #${currentIndex} | Size=${wavBuffer.byteLength} bytes | Duration=${(mergedAudio.length / SAMPLE_RATE).toFixed(2)}s`);

                        if (chunkCallbackRef.current) {
                            chunkCallbackRef.current(currentIndex, wavBuffer, mergedAudio);
                        }

                        speechBufferRef.current = [];
                        // å¦‚æœæ˜¯åŠ é€Ÿæ¨¡å¼ï¼Œé‡ç½®é™éŸ³å¸§ï¼›å›ºå®šæ¨¡å¼ä¸‹ä¹Ÿé‡ç½®
                        silenceFramesRef.current = 0;
                    } else {
                        logDebug(`[Warn] shouldCut=true but buffer empty`);
                    }
                }
            }
        } catch (err) {
            console.error('VAD å¤„ç†é”™è¯¯:', err);
            logDebug(`[Error] VAD Processing: ${err}`);
        }
    }, [isSpeaking, logDebug]);

    // å¼€å§‹å½•éŸ³
    const startRecording = useCallback(async (): Promise<boolean> => {
        if (!isVADReady) {
            setError('VAD æ¨¡å‹å°šæœªåŠ è½½');
            return false;
        }

        try {
            setError(null);
            chunkIndexRef.current = 0;
            setChunkCount(0);
            setDuration(0);
            speechBufferRef.current = [];
            silenceFramesRef.current = 0;
            speechFramesRef.current = 0;

            // è·å–éº¦å…‹é£
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    sampleRate: SAMPLE_RATE,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                }
            });
            streamRef.current = stream;
            setStreamState(stream); // Trigger re-render to share stream with Waveform

            // Start Temp Recording Session
            try {
                const sessionRes = await window.ipcRenderer.invoke('start-temp-recording');
                if (sessionRes.success && sessionRes.sessionId) {
                    console.log(`ğŸ“ Temp recording started: ${sessionRes.sessionId}`);
                    sessionIdRef.current = sessionRes.sessionId;
                    setSessionId(sessionRes.sessionId);
                } else {
                    console.error('Failed to start temp recording:', sessionRes.error);
                }
            } catch (e) {
                console.error('IPC error starting temp recording:', e);
            }

            // åˆ›å»º AudioContext
            const audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
            audioContextRef.current = audioContext;

            const source = audioContext.createMediaStreamSource(stream);

            // ä½¿ç”¨ ScriptProcessorNode è¿›è¡Œå®æ—¶å¤„ç†
            const processor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (event) => {
                const inputData = event.inputBuffer.getChannelData(0);
                processAudioChunk(new Float32Array(inputData));
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            setIsRecording(true);

            // å¯åŠ¨æ—¶é•¿è®¡æ—¶å™¨
            const startTime = Date.now();
            durationTimerRef.current = window.setInterval(() => {
                setDuration(Math.floor((Date.now() - startTime) / 1000));
            }, 1000);

            console.log('ğŸ¤ FunASR VAD å½•éŸ³å·²å¯åŠ¨');
            return true;

        } catch (err) {
            console.error('å½•éŸ³å¯åŠ¨å¤±è´¥:', err);
            setError('éº¦å…‹é£è®¿é—®å¤±è´¥: ' + (err as Error).message);
            return false;
        }
    }, [isVADReady, processAudioChunk]);

    // åœæ­¢å½•éŸ³
    const stopRecording = useCallback(() => {
        if (durationTimerRef.current) {
            clearInterval(durationTimerRef.current);
            durationTimerRef.current = null;
        }

        if (processorRef.current) {
            processorRef.current.disconnect();
            processorRef.current = null;
        }

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
            setStreamState(null); // Clear shared stream state
        }

        if (audioContextRef.current) {
            audioContextRef.current.close();
            audioContextRef.current = null;
        }

        // å¤„ç†å‰©ä½™çš„è¯­éŸ³ç¼“å†²
        if (speechBufferRef.current.length > 0) {
            const totalLength = speechBufferRef.current.reduce((sum, buf) => sum + buf.length, 0);
            const mergedAudio = new Float32Array(totalLength);
            let offset = 0;
            for (const buf of speechBufferRef.current) {
                mergedAudio.set(buf, offset);
                offset += buf.length;
            }

            // CHECK MODE: If unlimited, do not slice. Send ONE BIG CHUNK.
            const currentMode = vadModeRef.current;

            if (currentMode === 'unlimited') {
                // --- UNLIMITED MODE: SINGLE CHUNK ---
                const wavBuffer = float32ToWav(mergedAudio, SAMPLE_RATE);
                const currentIndex = chunkIndexRef.current;
                chunkIndexRef.current++;
                setChunkCount((prev: number) => prev + 1);

                console.log(`ğŸµ [Unlimited] å‘é€å®Œæ•´å½•éŸ³, å¤§å°: ${wavBuffer.byteLength} bytes, æ—¶é•¿: ${(mergedAudio.length / SAMPLE_RATE).toFixed(2)}s`);

                if (chunkCallbackRef.current) {
                    chunkCallbackRef.current(currentIndex, wavBuffer, mergedAudio);
                }

            } else {
                // --- OTHER MODES: Keep safety slicing if very long (fallback) ---
                // But generally 'vad' and 'time_limit' should have cut already.
                // This is just cleanup for leftovers.

                // Slice into smaller chunks for transmission (e.g. 10s = 16000 * 10 = 160000 samples)
                const MAX_CHUNK_SAMPLES = 16000 * 10; // 10 seconds per chunk

                for (let i = 0; i < mergedAudio.length; i += MAX_CHUNK_SAMPLES) {
                    const end = Math.min(i + MAX_CHUNK_SAMPLES, mergedAudio.length);
                    const chunkPCM = mergedAudio.slice(i, end);
                    const wavBuffer = float32ToWav(chunkPCM, SAMPLE_RATE);

                    const currentIndex = chunkIndexRef.current;
                    chunkIndexRef.current++;
                    setChunkCount((prev: number) => prev + 1);

                    console.log(`ğŸµ [Cleanup] å‘é€å‰©ä½™éŸ³é¢‘ç‰‡æ®µ #${currentIndex}, å¤§å°: ${wavBuffer.byteLength} bytes`);

                    if (chunkCallbackRef.current) {
                        chunkCallbackRef.current(currentIndex, wavBuffer, chunkPCM);
                    }
                }
            }

            speechBufferRef.current = [];
        } else {
            console.warn('âš ï¸ åœæ­¢å½•éŸ³æ—¶ç¼“å†²åŒºä¸ºç©º (VAD å¯èƒ½æœªæ£€æµ‹åˆ°è¯­éŸ³)');
        }
        vadRef.current?.reset();
        setIsRecording(false);
        setIsSpeaking(false);
        isSpeakingRef.current = false;

        console.log('ğŸ›‘ å½•éŸ³å·²åœæ­¢');

        sessionIdRef.current = null;
    }, []);

    // æ¸…ç†
    useEffect(() => {
        return () => {
            if (durationTimerRef.current) {
                clearInterval(durationTimerRef.current);
            }
            streamRef.current?.getTracks().forEach(track => track.stop());
            audioContextRef.current?.close();
        };
    }, []);

    return {
        isRecording,
        isVADReady,
        isSpeaking,
        chunkCount,
        duration,
        error,
        startRecording,
        stopRecording,
        onChunkReady,
        setVadMode,
        setTimeLimit,
        sessionId,
        stream: streamState // Reactive state for Waveform visualization
    };
}
