/**
 * FunASR FSMN-VAD ONNX æ¨ç†å™¨
 * 
 * ä½¿ç”¨é˜¿é‡Œè¾¾æ‘©é™¢çš„ FSMN-VAD æ¨¡å‹è¿›è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹
 * 
 * æµå¼å¤„ç†è¯´æ˜ï¼š
 * - ç»´æŠ¤ sample bufferï¼šç´¯ç§¯ PCM æ ·æœ¬ï¼Œåªå¤„ç†å®Œæ•´å¸§ï¼Œä¿ç•™ overlap
 * - ç»´æŠ¤ feature bufferï¼šç´¯ç§¯ Log Mel ç‰¹å¾å¸§ï¼Œç”¨äº LFR ä¸Šä¸‹æ–‡
 * - æ¨¡å‹ cacheï¼šFSMN éšè—çŠ¶æ€ï¼Œè·¨å—ä¿æŒ
 */
import * as ort from 'onnxruntime-web';
import {
    applyCMVN,
    parseCMVN,
    computeLogMelFrames,

    FRAME_LENGTH,
    FRAME_SHIFT,
    LFR_CONTEXT
} from './fbank';

interface VADSegment {
    start: number;  // å¼€å§‹æ—¶é—´ (ms)
    end: number;    // ç»“æŸæ—¶é—´ (ms)
}

interface VADConfig {
    speechThreshold: number;      // è¯­éŸ³æ¦‚ç‡é˜ˆå€¼
    silenceThreshold: number;     // é™éŸ³æ¦‚ç‡é˜ˆå€¼
    minSpeechDurationMs: number;  // æœ€å°è¯­éŸ³æ®µæ—¶é•¿
    minSilenceDurationMs: number; // æœ€å°é™éŸ³æ—¶é•¿ï¼ˆè§¦å‘åˆ†å—ï¼‰
}

const DEFAULT_CONFIG: VADConfig = {
    speechThreshold: 0.5,
    silenceThreshold: 0.35,
    minSpeechDurationMs: 200,
    minSilenceDurationMs: 500,
};

export class FunASRVAD {
    private session: ort.InferenceSession | null = null;
    private cmvnMeans: Float32Array | null = null;
    private cmvnScales: Float32Array | null = null;
    private config: VADConfig;
    private isReady = false;

    // è°ƒè¯•æ—¥å¿—å›è°ƒ (ç”¨äºå†™å…¥æ–‡ä»¶)
    private debugLog: ((msg: string) => void) | null = null;

    // FSMN æ¨¡å‹éšè—çŠ¶æ€ç¼“å­˜
    private cache: Record<string, Float32Array> = {};

    // ========== æµå¼ç¼“å†²åŒº (Streaming Buffers) ==========
    // Sample Buffer: ç´¯ç§¯ PCM æ ·æœ¬ï¼Œä¿ç•™ overlap
    private sampleBuffer: Float32Array = new Float32Array(0);
    // Feature Buffer: ç´¯ç§¯ Log Mel å¸§ï¼Œç”¨äº LFR ä¸Šä¸‹æ–‡
    private featureBuffer: Float32Array[] = [];
    // å·²å¤„ç†çš„ LFR å¸§æ•°ï¼ˆç”¨äºè®¡ç®—æ–°ç‰¹å¾çš„èµ·å§‹ç´¢å¼•ï¼‰
    private processedLFRFrames: number = 0;

    constructor(config: Partial<VADConfig> = {}) {
        this.config = { ...DEFAULT_CONFIG, ...config };
    }

    /**
     * è®¾ç½®è°ƒè¯•æ—¥å¿—å›è°ƒ
     */
    setDebugLogger(logger: (msg: string) => void): void {
        this.debugLog = logger;
    }

    private log(msg: string): void {
        if (this.debugLog) {
            this.debugLog(msg);
        }
        console.log(msg);
    }

    /**
     * åˆå§‹åŒ– VAD æ¨¡å‹
     */
    async init(modelPath: string = '/models/model.onnx', mvnPath: string = '/models/vad.mvn'): Promise<void> {
        try {
            console.log('ğŸ”„ åŠ è½½ FunASR VAD æ¨¡å‹...');

            // é…ç½® WASM è·¯å¾„ (Electron ç¯å¢ƒä¸‹éœ€è¦æ˜ç¡®æŒ‡å®š)
            ort.env.wasm.wasmPaths = '/onnx/';

            // åŠ è½½ ONNX æ¨¡å‹
            this.session = await ort.InferenceSession.create(modelPath, {
                executionProviders: ['wasm'],
                graphOptimizationLevel: 'all',
            });

            // åŠ è½½ CMVN å‚æ•°
            const mvnResponse = await fetch(mvnPath);
            const mvnContent = await mvnResponse.text();
            const { means, scales } = parseCMVN(mvnContent);
            this.cmvnMeans = means;
            this.cmvnScales = scales;

            // [DEBUG] Verify CMVN stats
            const meansSum = means.reduce((a, b) => a + b, 0);
            const scalesSum = scales.reduce((a, b) => a + b, 0);
            console.log(`ğŸ“Š [VAD Init] CMVN Loaded: MeansSum=${meansSum.toFixed(2)}, ScalesSum=${scalesSum.toFixed(2)}, Dim=${means.length}`);

            // è·å–æ¨¡å‹è¾“å…¥/è¾“å‡ºä¿¡æ¯
            console.log('ğŸ“Š æ¨¡å‹è¾“å…¥:', this.session.inputNames);
            console.log('ğŸ“Š æ¨¡å‹è¾“å‡º:', this.session.outputNames);
            console.log('ğŸ“Š è¾“å‡ºæ•°é‡:', this.session.outputNames.length);

            // å¦‚æœæœ‰å¤šä¸ªè¾“å‡ºï¼Œæ‰“å°æ‰€æœ‰è¾“å‡ºåç§°
            for (let i = 0; i < this.session.outputNames.length; i++) {
                console.log(`ğŸ“Š è¾“å‡º[${i}]: ${this.session.outputNames[i]}`);
            }

            this.isReady = true;
            console.log('âœ… FunASR VAD æ¨¡å‹åŠ è½½å®Œæˆ');

        } catch (error) {
            console.error('âŒ FunASR VAD æ¨¡å‹åŠ è½½å¤±è´¥:', error);
            throw error;
        }
    }

    /**
     * æ£€æµ‹è¯­éŸ³æ´»åŠ¨ (æµå¼å¤„ç†)
     * @param audioData 16kHz å•å£°é“ Float32 éŸ³é¢‘æ•°æ®
     * @returns è¯­éŸ³æ¦‚ç‡ (0-1)ï¼Œå¦‚æœç§¯æ”’çš„éŸ³é¢‘ä¸è¶³ä»¥äº§ç”Ÿæ–°å¸§ï¼Œè¿”å› 0
     */
    async detect(audioData: Float32Array): Promise<number> {
        if (!this.isReady || !this.session || !this.cmvnMeans || !this.cmvnScales) {
            throw new Error('VAD æ¨¡å‹æœªåˆå§‹åŒ–');
        }

        // 1. æ‹¼æ¥ Sample Buffer
        const newBuffer = new Float32Array(this.sampleBuffer.length + audioData.length);
        newBuffer.set(this.sampleBuffer);
        newBuffer.set(audioData, this.sampleBuffer.length);

        // 2. è®¡ç®—å¯ç”Ÿæˆçš„å®Œæ•´å¸§æ•°
        // æ¯ä¸€å¸§é•¿ 400 (25ms), æ­¥é•¿ 160 (10ms)
        // èƒ½å¤Ÿç”Ÿæˆçš„å¸§æ•° Næ»¡è¶³: (N-1)*160 + 400 <= total_samples
        const numNewFrames = Math.floor((newBuffer.length - FRAME_LENGTH) / FRAME_SHIFT) + 1;

        if (numNewFrames <= 0) {
            // æ•°æ®ä¸è¶³ä¸€å¸§ï¼Œä»…æ›´æ–° buffer
            this.sampleBuffer = newBuffer;
            return -1; // è¿”å› -1 è¡¨ç¤ºæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­ï¼ˆåŒºåˆ«äº 0 æ¦‚ç‡ï¼‰
        }

        // 3. æå–ç”¨äºè®¡ç®—ç‰¹å¾çš„æœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µ
        // æˆ‘ä»¬åªå¤„ç†å®Œæ•´å¸§ï¼Œå‰©ä½™çš„æ ·æœ¬ç•™ç»™ä¸‹ä¸€æ¬¡
        const processedSampleCount = (numNewFrames - 1) * FRAME_SHIFT + FRAME_LENGTH;
        const processAudio = newBuffer.slice(0, processedSampleCount);

        // æ›´æ–° sample buffer: å»æ‰å·²ç»â€œå®Œå…¨æ¶ˆè€—â€å¹¶äº§ç”Ÿå¸§ç§»çš„éƒ¨åˆ†
        // å®é™…ä¸Šï¼Œæ¯äº§ç”Ÿä¸€å¸§ï¼Œæˆ‘ä»¬â€œå‰è¿›â€äº† FRAME_SHIFTã€‚
        // æˆ‘ä»¬ä¿ç•™çš„æ•°æ®åº”è¯¥æ˜¯ï¼šnewBuffer.slice(numNewFrames * FRAME_SHIFT)
        // ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºç¬¬ N+1 å¸§éœ€è¦ä» N * FRAME_SHIFT å¼€å§‹ï¼Œé•¿åº¦ FRAME_LENGTHã€‚
        // åªè¦ buffer é‡Œæœ‰ FRAME_LENGTH é•¿åº¦çš„æ•°æ®ï¼Œå°±èƒ½äº§ç”Ÿä¸‹ä¸€å¸§ã€‚
        this.sampleBuffer = newBuffer.slice(numNewFrames * FRAME_SHIFT);

        // ä½¿ç”¨ fbank.ts ä¸­å¯¼å‡ºçš„æµå¼å‡½æ•° computeLogMelFrames
        const logMelFrames = computeLogMelFrames(processAudio);


        if (logMelFrames.length === 0) return -1;

        // 4. æ›´æ–° Feature Buffer (å­˜å‚¨æœªå½’ä¸€åŒ–çš„ LogMel å¸§)
        this.featureBuffer.push(...logMelFrames);

        // 5. è®¡ç®— LFR ç‰¹å¾
        // LFR éœ€è¦ 5 å¸§ä¸Šä¸‹æ–‡ (LFR_Context=5)
        // æˆ‘ä»¬éœ€è¦ä»ä¸Šæ¬¡å¤„ç†åˆ°çš„ä½ç½®ç»§ç»­è®¡ç®—
        // æ‰€æœ‰çš„ available frames = this.featureBuffer
        // ä¸Šæ¬¡å¤„ç†äº† this.processedLFRFrames ä¸ª LFR è¾“å‡º
        // æ¯ä¸€ä¸ª LFR è¾“å‡ºæ¶ˆè€— 1 ä¸ª shift (LFR_N=1)ã€‚
        // ä½†æ˜¯ LFR éœ€è¦ T+LFR_M-1 ä¸ªå¸§æ‰èƒ½äº§ç”Ÿ T ä¸ªè¾“å‡ºï¼Ÿ
        // ä¸ï¼ŒLFR åªè¦æœ‰ 5 å¸§å°±èƒ½äº§ç”Ÿç¬¬ 1 ä¸ª outputã€‚æœ‰ 6 å¸§äº§ç”Ÿç¬¬ 2 ä¸ª...
        // Output[i] needs Input[i...i+4]

        const startIndex = this.processedLFRFrames;

        // èƒ½å¤Ÿäº§ç”Ÿçš„ LFR å¸§æ•°
        // å‡è®¾ featureBuffer é•¿åº¦ä¸º Lã€‚æˆ‘ä»¬éœ€è¦ idx, idx+1... idx+4 å­˜åœ¨
        // æœ€åå¯ç”¨çš„ idx æ»¡è¶³ idx + 4 < L  => idx < L - 4
        // æ‰€ä»¥èƒ½äº§ç”Ÿçš„æœ€å¤§ç´¢å¼•æ˜¯ L - 5ã€‚ æ€»å…±èƒ½äº§ç”Ÿ L - 4 ä¸ªï¼Ÿ
        // ç­‰ç­‰ï¼ŒFunASR çš„ LFR æ˜¯æ‹¼æ¥ã€‚
        // è®©æˆ‘ä»¬å¤ç”¨ computeLFRFromFrames çš„é€»è¾‘ï¼Œå®ƒä¼š clampã€‚
        // ä½†æ˜¯æµå¼å¤„ç†ä¸åº”è¯¥ clamp åˆ°æœªæ¥ï¼Œè€Œåº”è¯¥ç­‰å¾…æœªæ¥ã€‚
        // åªæœ‰å½“æœ‰è¶³å¤Ÿçš„æœªæ¥å¸§æ—¶æ‰è®¡ç®—ã€‚
        // ä¿®æ­£é€»è¾‘ï¼šåªæœ‰å½“ buffer ä¸­æœ‰è¶³å¤Ÿçš„å¸§ï¼ˆè‡³å°‘ LFR_CONTEXT å¸§ï¼‰æ—¶æ‰å¼€å§‹è®¡ç®—

        // å®é™…æ¯æ¬¡æˆ‘ä»¬åªéœ€è¦è®¡ç®— *æ–°* äº§ç”Ÿçš„ LFR å¸§
        // æ–°å¢äº† logMelFrames.length ä¸ªåŸºç¡€å¸§ã€‚
        // æˆ‘ä»¬å°è¯•ä» processedLFRFrames å¼€å§‹è®¡ç®—ï¼Œç›´åˆ°æ— æ³•æ»¡è¶³ 5 å¸§ä¸Šä¸‹æ–‡ä¸ºæ­¢(æˆ–è€…ä½¿ç”¨ clamp ç­–ç•¥ï¼Œä½†æµå¼æœ€å¥½ä¸è¦ clamp æœªæ¥)
        // è§‚çœ‹åŸ `applyLFR` å®ç°ï¼šå®ƒå¯¹æœ«å°¾è¿›è¡Œäº† clamp (å¤åˆ¶æœ€åä¸€å¸§)ã€‚
        // è¿™åœ¨æµå¼ä¸­æ˜¯å±é™©çš„ï¼Œå› ä¸ºå¯èƒ½ä¼šå¯¼è‡´"é¢„æµ‹æœªæ¥"æ˜¯é™æ­¢çš„ã€‚
        // ä¸¥è°¨çš„åšæ³•ï¼šç­‰å¾…ã€‚
        // ä½†æ˜¯ä¸ºäº†ä¿æŒç®€å•ä¸”è·ŸåŸé€»è¾‘ä¸€è‡´ï¼ˆåŸé€»è¾‘æ˜¯å®æ—¶ï¼‰ï¼Œæˆ‘ä»¬æš‚ä¸”å…è®¸ clampï¼Œå¦‚æœä¸å…è®¸ clamp ä¼šå¯¼è‡´å»¶è¿Ÿå¢åŠ  (4å¸§ = 40ms)ã€‚
        // è€ƒè™‘åˆ° 40ms å»¶è¿Ÿå¯ä»¥æ¥å—ï¼Œæˆ‘ä»¬é‡‡ç”¨ "ç­‰å¾…æ¨¡å¼"ï¼Œå³ä¸ clamp æœ€åå‡ å¸§ï¼Œç•™ä½œ bufferã€‚

        // ä¿®æ­£ï¼šæˆ‘ä»¬é‡æ–°å®ç°ä¸€ä¸ªç®€å•çš„ check
        // éœ€è¦ Input[t + 0] ... Input[t + 4]
        // æ‰€ä»¥æˆ‘ä»¬éœ€è¦ featureBuffer.length >= t + 5
        const maxLfrIndex = this.featureBuffer.length - LFR_CONTEXT;

        if (maxLfrIndex < startIndex) {
            // æ•°æ®ä¸å¤Ÿäº§ç”Ÿæ–°çš„ LFR å¸§ (æ—  padding)
            // æ­¤æ—¶ä¸è¿›è¡Œæ¨ç†ï¼Œç­‰å¾…æ›´å¤šæ•°æ®
            return -1; // è¿”å› -1 è¡¨ç¤ºæ•°æ®ä¸è¶³
        }

        // æˆªå–éœ€è¦è®¡ç®—çš„éƒ¨åˆ†
        // ä¸ºäº†è®¡ç®— [startIndex ... maxLfrIndex] çš„ LFRï¼Œæˆ‘ä»¬éœ€è¦ featureBuffer
        // æˆ‘ä»¬ç›´æ¥è°ƒç”¨ computeLFRFromFrames, ä½†æ˜¯å‘Šè¯‰å®ƒåªè®¡ç®—åˆ° maxLfrIndex
        // æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ slice å—ï¼Ÿ 
        // computeLFRFromFrames ä¼šè®¡ç®—æ‰€æœ‰ start åˆ° endã€‚
        // æˆ‘ä»¬è‡ªå·±æ‰‹åŠ¨å®ç°å¾ªç¯å§ï¼Œæ›´å¯æ§ã€‚

        const lfrFeatures: Float32Array[] = [];
        const featureDim = 80; // Mel dim

        for (let i = startIndex; i <= maxLfrIndex; i++) {
            const lfrFrame = new Float32Array(featureDim * LFR_CONTEXT);
            for (let j = 0; j < LFR_CONTEXT; j++) {
                lfrFrame.set(this.featureBuffer[i + j], j * featureDim);
            }
            lfrFeatures.push(lfrFrame);
        }

        this.processedLFRFrames += lfrFeatures.length;

        // æ¸…ç† Feature Buffer
        // æˆ‘ä»¬åªéœ€è¦ä¿ç•™æœ€å LFR_CONTEXT - 1 ä¸ªå¸§ä¾›ä¸‹æ¬¡ä½¿ç”¨
        // ä¹Ÿå°±æ˜¯ processedLFRFrames æŒ‡å‘çš„é‚£ä¸ªä½ç½®çš„å‰é¢ 4 ä¸ª
        // æ–°çš„ start index å°†æ˜¯ this.processedLFRFrames
        // æˆ‘ä»¬éœ€è¦ä¿ç•™ this.featureBuffer[this.processedLFRFrames ... ] ä»¥åŠå‰é¢ 4 ä¸ª?
        // ä¸ï¼ŒprocessedLFRFrames æ˜¯æŒ‡"ä¸‹ä¸€ä¸ªè¦è®¡ç®—çš„ LFR ç´¢å¼•"ã€‚
        // è®¡ç®— Output[next] éœ€è¦ Input[next] ... Input[next+4]
        // æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¿ç•™ Input[next] åŠå…¶ä¹‹åçš„æ‰€æœ‰å¸§ã€‚
        // ä»¥å‰çš„å¸§ (0 ... next-1) å¯ä»¥ä¸¢å¼ƒå—ï¼Ÿ æ˜¯çš„ã€‚
        // è®©æˆ‘ä»¬æ‰§è¡Œæ¸…ç†ä»¥é˜²æ­¢å†…å­˜æ³„æ¼
        const keepIndex = this.processedLFRFrames;
        if (keepIndex > 0) {
            this.featureBuffer = this.featureBuffer.slice(keepIndex);
            this.processedLFRFrames = 0; // é‡ç½®ç´¢å¼•ï¼Œå› ä¸º buffer è¢«åˆ‡æ–­äº†
        }

        if (lfrFeatures.length === 0) return -1;

        // 6. åº”ç”¨ CMVN åˆ° LFR ç‰¹å¾ (400ç»´)
        const cmvnFeatures = lfrFeatures.map(frame =>
            applyCMVN(frame, this.cmvnMeans!, this.cmvnScales!)
        );

        // 7. å‡†å¤‡ ONNX è¾“å…¥
        const numInferFrames = cmvnFeatures.length;
        const totalDim = 400; // 80 * 5
        const inputDataArray = new Float32Array(numInferFrames * totalDim);

        // Debug: Check first frame stats (Increased sampling to 5% for debug)
        if (numInferFrames > 0 && Math.random() < 0.05) {
            const firstFrame = cmvnFeatures[0];
            let min = Infinity, max = -Infinity, avg = 0;
            for (let val of firstFrame) {
                if (val < min) min = val;
                if (val > max) max = val;
                avg += val;
            }
            avg /= firstFrame.length;
            console.log(`ğŸ“Š [VAD Input Test] Shape=${numInferFrames}x${totalDim}, Min=${min.toFixed(4)}, Max=${max.toFixed(4)}, Avg=${avg.toFixed(4)}`);
        }

        for (let k = 0; k < numInferFrames; k++) {
            inputDataArray.set(cmvnFeatures[k], k * totalDim);
        }

        const inputTensor = new ort.Tensor('float32', inputDataArray, [1, numInferFrames, totalDim]);

        // 8. è¿è¡Œæ¨ç†
        const feeds: Record<string, ort.Tensor> = {};
        feeds[this.session.inputNames[0]] = inputTensor;

        // å¤„ç† Cache (FSMN çŠ¶æ€)
        const CACHE_SHAPE: [number, number, number, number] = [1, 128, 19, 1];
        const CACHE_SIZE = 1 * 128 * 19 * 1;

        for (let i = 1; i < this.session.inputNames.length; i++) {
            const inputName = this.session.inputNames[i];
            if (this.cache[inputName]) {
                feeds[inputName] = new ort.Tensor('float32', this.cache[inputName], CACHE_SHAPE);
            } else {
                feeds[inputName] = new ort.Tensor('float32', new Float32Array(CACHE_SIZE), CACHE_SHAPE);
            }
        }

        const results = await this.session.run(feeds);

        // 9. æ›´æ–° Cache
        for (let i = 1; i < this.session.outputNames.length; i++) {
            const outputKey = this.session.outputNames[i];
            if (i < this.session.inputNames.length) {
                const inputKey = this.session.inputNames[i];
                this.cache[inputKey] = results[outputKey].data as Float32Array;
            }
        }

        // 10. è§£æç»“æœ
        const outputName = this.session.outputNames[0];
        const outputData = results[outputName].data as Float32Array;

        // [DEBUG] è¾“å‡ºæ¨¡å‹åŸå§‹æ•°æ®åˆ°æ–‡ä»¶æ—¥å¿—
        this.log(`[VAD-Model] dims=[${results[outputName].dims}], len=${outputData.length}, first6=[${Array.from(outputData.slice(0, 6)).map(v => v.toFixed(4)).join(', ')}]`);


        let speechProb = 0;

        // âš ï¸ FALLBACK: å½“å‰ ONNX æ¨¡å‹ç¼ºå°‘åˆ†ç±»å±‚ï¼Œ248 ç»´è¾“å‡ºæ— æ³•æ­£ç¡®è§£æ
        // ä½¿ç”¨æŒ¯å¹…é˜ˆå€¼æ£€æµ‹ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆ
        // æœªæ¥éœ€è¦è·å–å®Œæ•´çš„ FSMN-VAD æ¨¡å‹æˆ–ä½¿ç”¨ Silero VAD

        const outputDim = 248;
        const numFrames = Math.floor(outputData.length / outputDim);

        if (numFrames > 0) {
            // æ–¹æ¡ˆï¼šä½¿ç”¨æ¯å¸§æ‰€æœ‰ç»´åº¦çš„å¹³å‡å€¼ä½œä¸ºæ´»åŠ¨åº¦æŒ‡æ ‡
            for (let f = 0; f < numFrames; f++) {
                const frameStart = f * outputDim;
                let frameSum = 0;
                for (let d = 0; d < outputDim; d++) {
                    frameSum += Math.abs(outputData[frameStart + d]);
                }
                const frameAvg = frameSum / outputDim;

                // ä½¿ç”¨é˜ˆå€¼ï¼šå¹³å‡å€¼ > 0.01 è®¤ä¸ºæ˜¯è¯­éŸ³
                // å¹¶ç”¨ sigmoid å¹³æ»‘
                const logit = (frameAvg - 0.015) * 200; // ä»¥ 0.015 ä¸ºä¸­å¿ƒï¼Œæ”¾å¤§å·®å¼‚
                const frameSpeechProb = 1 / (1 + Math.exp(-logit));
                speechProb += frameSpeechProb;
            }

            const finalProb = speechProb / numFrames;
            this.log(`[VAD-AmplitudeFallback] numFrames=${numFrames}, avgProb=${finalProb.toFixed(4)}`);
            return finalProb;
        } else {
            this.log(`[VAD-Error] No frames in output`);
            return 0;
        }
    }

    /**
     * å¤„ç†éŸ³é¢‘æµï¼Œè¿”å›è¯­éŸ³æ®µ
     */
    async processAudio(audioData: Float32Array): Promise<VADSegment[]> {
        if (!this.isReady) {
            throw new Error('VAD æ¨¡å‹æœªåˆå§‹åŒ–');
        }

        const segments: VADSegment[] = [];
        const frameShiftMs = 10;
        const windowMs = 200; // æ¯æ¬¡å¤„ç† 200ms

        const samplesPerWindow = Math.floor(16000 * windowMs / 1000);
        const samplesPerShift = Math.floor(16000 * frameShiftMs / 1000);

        let isSpeaking = false;
        let speechStart = 0;
        let silenceFrames = 0;
        let speechFrames = 0;

        for (let i = 0; i + samplesPerWindow <= audioData.length; i += samplesPerShift) {
            const window = audioData.slice(i, i + samplesPerWindow);
            const speechProb = await this.detect(window);
            const currentTimeMs = Math.floor(i * 1000 / 16000);

            if (speechProb >= this.config.speechThreshold) {
                speechFrames++;
                silenceFrames = 0;

                if (!isSpeaking && speechFrames * frameShiftMs >= this.config.minSpeechDurationMs) {
                    isSpeaking = true;
                    speechStart = currentTimeMs - this.config.minSpeechDurationMs;
                }
            } else if (speechProb < this.config.silenceThreshold) {
                silenceFrames++;
                speechFrames = 0;

                if (isSpeaking && silenceFrames * frameShiftMs >= this.config.minSilenceDurationMs) {
                    segments.push({
                        start: speechStart,
                        end: currentTimeMs,
                    });
                    isSpeaking = false;
                }
            }
        }

        // å¤„ç†æœ«å°¾çš„è¯­éŸ³æ®µ
        if (isSpeaking) {
            segments.push({
                start: speechStart,
                end: Math.floor(audioData.length * 1000 / 16000),
            });
        }

        return segments;
    }

    /**
     * é‡ç½®çŠ¶æ€
     */
    reset(): void {
        this.cache = {};
        this.sampleBuffer = new Float32Array(0);
        this.featureBuffer = [];
        this.processedLFRFrames = 0;
    }

    /**
     * æ£€æŸ¥æ˜¯å¦å°±ç»ª
     */
    get ready(): boolean {
        return this.isReady;
    }
}
