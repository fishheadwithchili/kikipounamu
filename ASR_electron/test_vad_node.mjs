#!/usr/bin/env node

/**
 * FunASR VAD çœŸå®æ¨¡å‹è‡ªåŠ¨åŒ–æµ‹è¯•
 * ç›´æ¥åœ¨ Node.js ç¯å¢ƒä¸­è¿è¡Œï¼Œä½¿ç”¨çœŸå®çš„ ONNX æ¨¡å‹
 */

import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import * as ort from 'onnxruntime-node';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// VAD é…ç½®
const VAD_CONFIG = {
    speechThreshold: 0.1,
    silenceThreshold: 0.35,
    minSpeechDurationMs: 200,
    minSilenceDurationMs: 500,
};

const SAMPLE_RATE = 16000;
const BUFFER_SIZE = 2048;
const BUFFER_SIZE_MS = Math.floor(BUFFER_SIZE * 1000 / SAMPLE_RATE);

console.log('\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—');
console.log('â•‘     FunASR VAD çœŸå®æ¨¡å‹æµ‹è¯• (ONNX Runtime Node)       â•‘');
console.log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

// è¯»å– WAV æ–‡ä»¶
function readWavFile(filePath) {
    console.log(`ğŸ“‚ è¯»å–éŸ³é¢‘æ–‡ä»¶: ${filePath}`);

    const buffer = fs.readFileSync(filePath);
    const dataOffset = 44;
    const pcmData = buffer.slice(dataOffset);

    const samples = new Int16Array(
        pcmData.buffer,
        pcmData.byteOffset,
        pcmData.byteLength / 2
    );

    const float32 = new Float32Array(samples.length);
    for (let i = 0; i < samples.length; i++) {
        float32[i] = samples[i] / 32768.0;
    }

    const durationSec = float32.length / SAMPLE_RATE;
    const sizeMB = (buffer.length / 1024 / 1024).toFixed(2);
    console.log(`âœ… éŸ³é¢‘åŠ è½½å®Œæˆ:`);
    console.log(`   - æ ·æœ¬æ•°: ${float32.length.toLocaleString()}`);
    console.log(`   - æ—¶é•¿: ${durationSec.toFixed(2)}ç§’`);
    console.log(`   - æ–‡ä»¶å¤§å°: ${sizeMB}MB\n`);

    return float32;
}

// è¯»å– CMVN æ–‡ä»¶
function loadCMVN(cmvnPath) {
    console.log(`ğŸ“Š åŠ è½½ CMVN å½’ä¸€åŒ–å‚æ•°: ${cmvnPath}`);

    const content = fs.readFileSync(cmvnPath, 'utf-8');
    const lines = content.trim().split('\n');

    if (lines.length < 2) {
        throw new Error('Invalid CMVN file format');
    }

    const parseLine = (line) => {
        const values = line.trim().split(/\s+/).slice(1);
        return values.map(v => parseFloat(v));
    };

    const means = parseLine(lines[0]);
    const vars = parseLine(lines[1]);

    console.log(`âœ… CMVN åŠ è½½å®Œæˆ: ${means.length} ç»´ç‰¹å¾\n`);

    return { means, vars };
}

// æå– Fbank ç‰¹å¾ (ç®€åŒ–ç‰ˆï¼Œä¸æµè§ˆå™¨ç‰ˆæœ¬ä¸€è‡´)
function extractFbank(audioChunk, numMels = 80) {
    // è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„èƒ½é‡ç‰¹å¾ä½œä¸ºå ä½
    // å®é™…åº”ç”¨ä¸­ä¼šä½¿ç”¨å®Œæ•´çš„ Fbank å®ç°
    const feature = new Float32Array(numMels);

    // è®¡ç®—èƒ½é‡åˆ†å¸ƒ
    let totalEnergy = 0;
    for (let i = 0; i < audioChunk.length; i++) {
        totalEnergy += audioChunk[i] * audioChunk[i];
    }

    // ç®€åŒ–çš„é¢‘è°±èƒ½é‡åˆ†å¸ƒ
    for (let i = 0; i < numMels; i++) {
        const start = Math.floor(i * audioChunk.length / numMels);
        const end = Math.floor((i + 1) * audioChunk.length / numMels);

        let binEnergy = 0;
        for (let j = start; j < end; j++) {
            binEnergy += audioChunk[j] * audioChunk[j];
        }

        feature[i] = binEnergy / (end - start);
    }

    return feature;
}

// åº”ç”¨ CMVN å½’ä¸€åŒ–
function applyCMVN(feature, cmvn) {
    const normalized = new Float32Array(feature.length);
    for (let i = 0; i < feature.length; i++) {
        const mean = cmvn.means[i] || 0;
        const variance = cmvn.vars[i] || 1;
        normalized[i] = (feature[i] - mean) / Math.sqrt(variance);
    }
    return normalized;
}

// VAD æ¨ç†ç±»
class FunASRVADNode {
    constructor(config) {
        this.config = config;
        this.session = null;
        this.cmvn = null;
        this.cache = null;
        this.cacheSize = 5; // LFR 5 frames
    }

    async init(modelPath, cmvnPath) {
        console.log('ğŸ”„ åˆå§‹åŒ– FunASR VAD æ¨¡å‹...');
        console.log(`   - æ¨¡å‹è·¯å¾„: ${modelPath}`);
        console.log(`   - CMVN è·¯å¾„: ${cmvnPath}`);

        // åŠ è½½ CMVN
        this.cmvn = loadCMVN(cmvnPath);

        // åŠ è½½ ONNX æ¨¡å‹
        console.log('ğŸ”„ åŠ è½½ ONNX æ¨¡å‹...');
        this.session = await ort.InferenceSession.create(modelPath);

        console.log('âœ… ONNX æ¨¡å‹åŠ è½½å®Œæˆ');
        console.log(`   - è¾“å…¥èŠ‚ç‚¹: ${this.session.inputNames.join(', ')}`);
        console.log(`   - è¾“å‡ºèŠ‚ç‚¹: ${this.session.outputNames.join(', ')}\n`);

        // åˆå§‹åŒ–ç¼“å­˜
        this.cache = {
            features: [],
            fsmn: {}
        };
    }

    async detect(audioChunk) {
        // æå–ç‰¹å¾
        const feature = extractFbank(audioChunk);

        // åº”ç”¨ CMVN
        const normalized = applyCMVN(feature, this.cmvn);

        // æ·»åŠ åˆ°ç¼“å­˜
        this.cache.features.push(normalized);

        // ä¿æŒç¼“å­˜å¤§å°
        if (this.cache.features.length > this.cacheSize) {
            this.cache.features.shift();
        }

        // å¦‚æœç¼“å­˜æœªæ»¡ï¼Œè¿”å›ä½æ¦‚ç‡
        if (this.cache.features.length < this.cacheSize) {
            return 0.0;
        }

        // æ„å»ºè¾“å…¥å¼ é‡ [1, 5, 80]
        const inputData = new Float32Array(1 * this.cacheSize * 80);
        for (let i = 0; i < this.cacheSize; i++) {
            inputData.set(this.cache.features[i], i * 80);
        }

        const inputTensor = new ort.Tensor('float32', inputData, [1, this.cacheSize, 80]);

        // å‡†å¤‡æ‰€æœ‰è¾“å…¥ (åŒ…æ‹¬ cache)
        const feeds = {};
        feeds[this.session.inputNames[0]] = inputTensor; // 'speech'

        // FSMN Cache inputs (in_cache0, in_cache1, in_cache2, in_cache3)
        const CACHE_SHAPE = [1, 128, 19, 1];
        const CACHE_SIZE = 1 * 128 * 19 * 1;

        for (let i = 1; i < this.session.inputNames.length; i++) {
            const inputName = this.session.inputNames[i];
            if (this.cache.fsmn[inputName]) {
                feeds[inputName] = new ort.Tensor('float32', this.cache.fsmn[inputName], CACHE_SHAPE);
            } else {
                feeds[inputName] = new ort.Tensor('float32', new Float32Array(CACHE_SIZE), CACHE_SHAPE);
            }
        }

        // è¿è¡Œæ¨ç†
        const results = await this.session.run(feeds);

        // æ›´æ–° FSMN Cache
        for (let i = 1; i < this.session.outputNames.length; i++) {
            const outputKey = this.session.outputNames[i];
            if (i < this.session.inputNames.length) {
                const inputKey = this.session.inputNames[i];
                this.cache.fsmn[inputKey] = results[outputKey].data;
            }
        }

        // è·å–è¯­éŸ³æ¦‚ç‡
        const output = results[this.session.outputNames[0]];
        const outputData = output.data;

        // è¾“å‡º shape [1, numFrames, 2] => [silence_prob, speech_prob]
        // å–å¹³å‡è¯­éŸ³æ¦‚ç‡
        let speechProb = 0;
        const numFrames = this.cacheSize;

        if (outputData.length === numFrames * 2) {
            for (let i = 0; i < outputData.length; i += 2) {
                speechProb += outputData[i + 1]; // speech_prob at index 1
            }
            return speechProb / numFrames;
        } else {
            // Fallback
            for (let i = 0; i < outputData.length; i++) {
                speechProb += outputData[i];
            }
            return speechProb / outputData.length;
        }
    }

    reset() {
        this.cache = {
            features: [],
            fsmn: {}
        };
    }
}

// æ‰§è¡Œ VAD æµ‹è¯•
async function testVAD(audioData, modelPath, cmvnPath) {
    console.log('â•'.repeat(60));
    console.log('ğŸ¯ å¼€å§‹ VAD æµ‹è¯•\n');

    console.log('é…ç½®ä¿¡æ¯:');
    console.log(`  - è¯­éŸ³é˜ˆå€¼: ${VAD_CONFIG.speechThreshold}`);
    console.log(`  - é™éŸ³é˜ˆå€¼: ${VAD_CONFIG.silenceThreshold}`);
    console.log(`  - æœ€å°é™éŸ³æ—¶é•¿: ${VAD_CONFIG.minSilenceDurationMs}ms`);
    console.log(`  - ç¼“å†²åŒºå¤§å°: ${BUFFER_SIZE} (${BUFFER_SIZE_MS}ms)\n`);

    // åˆå§‹åŒ– VAD
    const vad = new FunASRVADNode(VAD_CONFIG);
    await vad.init(modelPath, cmvnPath);

    console.log('â•'.repeat(60));
    console.log('\nå¼€å§‹å¤„ç†éŸ³é¢‘...\n');

    let speechBuffer = [];
    let silenceFrames = 0;
    let speechFrames = 0;
    let isSpeaking = false;
    let chunkCount = 0;
    const chunks = [];

    const totalFrames = Math.floor(audioData.length / BUFFER_SIZE);
    let lastProgressUpdate = Date.now();
    let processedFrames = 0;

    for (let i = 0; i < totalFrames; i++) {
        const start = i * BUFFER_SIZE;
        const end = Math.min(start + BUFFER_SIZE, audioData.length);
        const chunk = audioData.slice(start, end);

        if (chunk.length < BUFFER_SIZE) break;

        // VAD æ£€æµ‹
        const speechProb = await vad.detect(chunk);

        processedFrames++;

        // æ¯ç§’æ›´æ–°ä¸€æ¬¡è¿›åº¦
        const now = Date.now();
        if (now - lastProgressUpdate > 1000) {
            const progress = (processedFrames / totalFrames * 100).toFixed(1);
            const currentTime = (i * BUFFER_SIZE / SAMPLE_RATE).toFixed(1);
            process.stdout.write(`\râ±ï¸  è¿›åº¦: ${progress}% | æ—¶é—´: ${currentTime}s | åˆ‡ç‰‡: ${chunkCount}   `);
            lastProgressUpdate = now;
        }

        if (speechProb >= VAD_CONFIG.speechThreshold) {
            // è¯­éŸ³
            speechFrames++;
            silenceFrames = 0;

            if (!isSpeaking && speechFrames >= 2) {
                isSpeaking = true;
                const timestamp = (i * BUFFER_SIZE / SAMPLE_RATE).toFixed(2);
                console.log(`\nğŸ”Š [${timestamp}s] å¼€å§‹è¯´è¯ (prob=${speechProb.toFixed(3)})`);
            }

            speechBuffer.push(chunk);

        } else if (speechProb < VAD_CONFIG.silenceThreshold) {
            // é™éŸ³
            silenceFrames++;
            speechFrames = 0;

            // VAD æ¨¡å¼ï¼šå§‹ç»ˆç¼“å†²
            speechBuffer.push(chunk);

            // æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡åˆ†
            const silenceDurationMs = silenceFrames * BUFFER_SIZE_MS;
            if (silenceDurationMs >= VAD_CONFIG.minSilenceDurationMs && isSpeaking) {
                const timestamp = (i * BUFFER_SIZE / SAMPLE_RATE).toFixed(2);

                if (speechBuffer.length > 0) {
                    const totalSamples = speechBuffer.reduce((sum, buf) => sum + buf.length, 0);
                    const durationSec = (totalSamples / SAMPLE_RATE).toFixed(2);
                    const sizeKB = (totalSamples * 4 / 1024).toFixed(1);

                    chunkCount++;
                    console.log(`ğŸ”‡ [${timestamp}s] æ£€æµ‹åˆ°é™éŸ³ ${silenceDurationMs}msï¼Œæ‰§è¡Œåˆ‡åˆ†`);
                    console.log(`âœ‚ï¸  éŸ³é¢‘å— #${chunkCount} | ${totalSamples} æ ·æœ¬ | ${durationSec}s | ${sizeKB}KB\n`);

                    chunks.push({
                        index: chunkCount,
                        timestamp: parseFloat(timestamp),
                        samples: totalSamples,
                        duration: parseFloat(durationSec),
                        sizeKB: parseFloat(sizeKB)
                    });

                    speechBuffer = [];
                    silenceFrames = 0;
                }

                isSpeaking = false;
            }
        }
    }

    console.log('\n');

    // å¤„ç†å‰©ä½™ç¼“å†²
    if (speechBuffer.length > 0) {
        const totalSamples = speechBuffer.reduce((sum, buf) => sum + buf.length, 0);
        const durationSec = (totalSamples / SAMPLE_RATE).toFixed(2);
        const sizeKB = (totalSamples * 4 / 1024).toFixed(1);

        chunkCount++;
        console.log(`âœ‚ï¸  [æœ€å] éŸ³é¢‘å— #${chunkCount} | ${totalSamples} æ ·æœ¬ | ${durationSec}s | ${sizeKB}KB\n`);

        chunks.push({
            index: chunkCount,
            timestamp: 0,
            samples: totalSamples,
            duration: parseFloat(durationSec),
            sizeKB: parseFloat(sizeKB)
        });
    }

    // æ˜¾ç¤ºæ€»ç»“
    console.log('â•'.repeat(60));
    console.log('\nâœ… æµ‹è¯•å®Œæˆï¼\n');
    console.log(`ç»Ÿè®¡ä¿¡æ¯:`);
    console.log(`  - æ€»åˆ‡ç‰‡æ•°: ${chunkCount}`);
    console.log(`  - æ€»å¸§æ•°: ${totalFrames.toLocaleString()}`);
    console.log(`  - éŸ³é¢‘æ—¶é•¿: ${(audioData.length / SAMPLE_RATE).toFixed(2)}ç§’`);

    if (chunks.length > 0) {
        const avgDuration = chunks.reduce((sum, c) => sum + c.duration, 0) / chunks.length;
        const minDuration = Math.min(...chunks.map(c => c.duration));
        const maxDuration = Math.max(...chunks.map(c => c.duration));

        console.log(`\nåˆ‡ç‰‡ç»Ÿè®¡:`);
        console.log(`  - å¹³å‡æ—¶é•¿: ${avgDuration.toFixed(2)}ç§’`);
        console.log(`  - æœ€çŸ­æ—¶é•¿: ${minDuration.toFixed(2)}ç§’`);
        console.log(`  - æœ€é•¿æ—¶é•¿: ${maxDuration.toFixed(2)}ç§’`);
    }

    console.log('\n' + 'â•'.repeat(60) + '\n');

    return chunks;
}

// ä¸»å‡½æ•°
async function main() {
    const audioPath = process.argv[2] || '/home/tiger/Projects/ASR_pc_front/recording/long_audio_test.wav';
    const modelPath = path.join(__dirname, 'public/models/model.onnx');
    const cmvnPath = path.join(__dirname, 'public/models/vad.mvn');

    // æ£€æŸ¥æ–‡ä»¶
    if (!fs.existsSync(audioPath)) {
        console.error(`âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: ${audioPath}`);
        process.exit(1);
    }

    if (!fs.existsSync(modelPath)) {
        console.error(`âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: ${modelPath}`);
        process.exit(1);
    }

    if (!fs.existsSync(cmvnPath)) {
        console.error(`âŒ CMVN æ–‡ä»¶ä¸å­˜åœ¨: ${cmvnPath}`);
        process.exit(1);
    }

    try {
        const audioData = readWavFile(audioPath);
        const chunks = await testVAD(audioData, modelPath, cmvnPath);

        console.log(`\nâœ… æµ‹è¯•æˆåŠŸå®Œæˆï¼å…±æ£€æµ‹åˆ° ${chunks.length} ä¸ªè¯­éŸ³ç‰‡æ®µã€‚\n`);

    } catch (err) {
        console.error('\nâŒ æµ‹è¯•å¤±è´¥:', err);
        console.error(err.stack);
        process.exit(1);
    }
}

main().catch(console.error);
