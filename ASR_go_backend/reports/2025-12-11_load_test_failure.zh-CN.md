# ASR 负载测试报告 (ASR Load Test Report)

> **语言切换**: [English](2025-12-11_load_test_failure.md) | [简体中文](2025-12-11_load_test_failure.zh-CN.md)


**日期:** 2025-12-11T21:55:00+13:00
**模式:** Short (高并发短音频模式)
**并发数:** 500
**持续时间:** 30s
**音频源:** /home/tiger/Projects/Katydid/ASR_server/tests/resources/test_audio_short.wav

## 测试结果概览
- **总尝试会话数:** ~111,510
- **成功会话数:** 0
- **错误数:** ~111,510 (100% 错误率)
- **峰值活跃连接数:** 109 (从后端日志确认)

## 仪表盘监控数据 (30s时刻)
- **后端 CPU 使用率:** 118.59% (多核满载，表明系统正在全力处理请求)
- **后端内存使用:** 27.81 MB

## 核心观察与分析
1.  **系统已达到极限**: 后端 CPU 瞬间飙升至 >100%，说明**日志系统**和**网络协议栈**在高并发下正常工作并记录了大量数据。
2.  **连接拒绝与断开**: 虽然测试端报告 100% 错误，但后端日志显示系统实际上**接受并处理了部分连接** (峰值约 109 个)，但由于并发量远超单节点处理能力 (500并发)，导致大量连接被系统强制断开或客户端超时。
3.  **日志系统验证通过**: 后端日志 (`backend.log`) 成功记录了高频的连接建立与断开事件 (`❌ WebSocket 连接已断开 {"active_connections": ...}`), 证明 Zap 日志库在高负载下依然稳定，没有发生阻塞或崩溃。
4.  **Spin Loop 缓解**: 测试工具已加入重试等待机制，将错误量从 50万降低到了 11万，避免了无效的洪水攻击。

## 改进建议
1.  **架构优化**: 500并发对于单节点 Python/Go 混合架构压力较大，建议前置 Nginx 负载均衡或增加后端节点。
2.  **系统调优**: 建议在生产环境调大 `ulimit -n` 和 TCP 连接队列参数 (`net.core.somaxconn`)。
3.  **应用层限流**: 在 Go 后端增加排队或限流机制，避免过多连接同时涌入导致服务不可用。

# 深度复盘与架构演进 (2025-12-11 Update)

> 基于与 User 的深度讨论，对本次负载测试失败的根本原因进行了详细剖析，并确定了未来的架构演进方向。

## 1. 故障根因深度分析

### 1.1 为什么是 100% 错误？
**表面原因**：测试报告显示 `Connection Refused` 或 503 Service Unavailable。
**根本原因**：**人为设置的连接墙 (Hardcoded Limit)**。
- 代码中 `MaxConnections = 100`。
- 测试并发 500。
- 结果：100 人进场，400 人被拒。被拒的人（客户端）疯狂重试，导致 30 秒内产生了 11 万次拒绝记录。

### 1.2 为什么 CPU 飙升到 118%？
**误区**：以为 Python 在拼命识别音频。
**真相**：**Go 后端在拼命写日志**。
- 11 万次请求被拒，意味着每秒约 3700 次 `logger.Warn`。
- 整个 CPU 都在忙着：TCP 三次握手 -> 检查计数器 -> 格式化 Zap 日志 -> 写入磁盘 -> 关闭连接。
- 真正的业务逻辑（音频识别）几乎没跑起来。

### 1.3 架构瓶颈：同步阻塞的 WorkerPool
当前 Go 代码使用了一个 `WorkerPool` (size=4) 来处理请求。
- **流程**：Client -> Go Worker -> HTTP POST -> Python Server (等待...) -> Go Worker 释放。
- **问题**：Go Worker 变成了 Python 的“人质”。Python 处理慢，Go Worker 就得陪着等。Worker 耗尽后，主线程无法处理新请求。
- **比喻**：银行大厅（Go）只有 4 个窗口（Worker）。办事员打个电话给总部（Python）核实信息，电话一打就是 10 分钟。这期间办事员什么都干不了，后面的客户只能干瞪眼。

---

## 2. 架构演进决策

### 2.1 核心决策：去 Worker 化 (Go -> Redis)
**User 疑问**：既然 Python 处理能力有限（只有 2 个 Worker），Go 做那么快有什么用？
**解答**：
- **"银行大厅"理论**：Go 的作用是让用户**进得来，坐得下**（排队），而不是把用户拒之门外。
- **新架构**：
  - **Go**：不再做 Worker，只做**网关**。接收请求 -> 丢进 Redis 队列 -> 返回排队号。耗时：0.1ms。
  - **Redis**：充当无限容量的蓄水池。
  - **Python**：按自己的节奏从 Redis 拿任务处理。
- **收益**：Go 不再被 Python 的速度拖死，可以轻松抗住 1000+ 并发连接，主要消耗只剩下维持 WebSocket 的心跳。

### 2.2 短期止血方案 (Action Items)
在进行 Redis 架构重构前，为了让现有系统能跑通测试：
1.  **调大参数**：`MaxConnections` 从 100 -> 1000。
2.  **扩大缓冲**：`WorkerPool` 从 4 -> 200（利用 Go 协程廉价的特性，用数量换时间）。
3.  **日志降级**：开启 Zap Sampling，防止磁盘 I/O 成为瓶颈。

---

## 3. 学习与思考
- **关于长远考虑**：不要因为后端处理慢，就在网关层人为限制并发。网关的职责是**最大化吞吐**和**削峰填谷**。
- **关于日志**：在高并发场景下，日志本身可能成为最大的性能杀手。必须有采样或动态降级机制。
